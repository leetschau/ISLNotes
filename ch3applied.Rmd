---
title: "Applied Exercises in Chapter 3 of ISL"
output: html_notebook
---

# Question 8

## 8a
```{r}
library(ISLR)
data(Auto)
fit <- lm(mpg ~ horsepower, data = Auto)
summary(fit)

predict(fit, data.frame(horsepower = c(98)), interval = 'confidence')
predict(fit, data.frame(horsepower = c(98)), interval = 'prediction')
```

i. There is a relationship between *mpg* and *horsepower* in data frame *Auto*, because the *p-value* of the t-statistic is less than 0.05.

ii. The $R^2$ statistic suggests *horsepower* explains 61% of *mpg*.

iii. Negative. For the coeffecient of *horsepower* is less than 0.

iv. $mpg = 39.9 - 0.158 \times horsepower = 39.9 - 0.158 \times 98 = 24.4$

The associated 95% confidence interval is $[23.97, 24.96]$, the prediction interval is $[14.81, 34.12]$.


## 8b
```{r}
plot(Auto$horsepower, Auto$mpg)
abline(fit)
```

## 8c
```{r}
plot(predict(fit), residuals(fit))
plot(predict(fit), rstudent(fit))
par(mfrow=c(2,2))
plot(fit)
```
The patterns in the *Residuals vs Fitted* graph shows there are some non-linearity in the relationship of *horsepower* and *mpg*.

# Question 9
## 9a and 9b
```{r}
plot(Auto) # equivalent to `pairs(Auto)`
cor(within(Auto, rm(name)))
```

## 9c
```{r}
mfit <- lm(mpg ~ . -name, data = Auto)
summary(mfit)
```

i. Yes, there is.

ii. *weight*, *year* and *origin*.

iii. Cars become more oil-efficient as the service life continue, the *mpg* increase about 0.75 per year.

## 9d
```{r}
par(mfrow=c(2,2))
plot(mfit)
par(mfrow=c(1,1))
plot(predict(mfit), rstudent(mfit))
abline(3, 0)
```
The residual plot suggests no **large** outliers. The leverage plot suggests the no. 14 observation have unusually high leverage.

In *rstudent* plot, we see there are about 4 outliers which `abs(x) > 3`, but not far from 3.

## 9e
From the result of (c).ii, I choose the following 3 predictors:
```{r}
imfit1 <- lm(mpg ~ weight * year * origin, data = Auto)
summary(imfit1)
```
The result shows all the interaction items have significant effect on *mpg*.

From results of (b), we find the largest 2 correlations: cylinders vs. displacement (0.95), weight vs. displacement (0.93). So we test their interaction effects:
```{r}
imfit2 <- lm(mpg ~ cylinders * displacement + displacement * weight, data = Auto)
summary(imfit2)
```

So the interaction between displacement and weight is significant, while that between cylinders and displacement is insignificant.

# Question 10
## 10a
```{r}
data("Carseats")
fit <- lm(Sales ~ Price + Urban + US, data = Carseats)
summary(fit)
```
## 10b
Expensive car seats have less sales than cheaper ones. Whether the store is at urban or not doesn't effect the sales. The stores in the U.S. have more sales than that outside the U.S.

## 10c
$$sales = \begin{cases}
13.04 - 0.05 \times price & \text{if US = 0}  \\
13.04 - 0.05 \times price + 1.20 & \text{if US = 1}
\end{cases}
$$

## 10d
*Price* and *US*.

## 10e
```{r}
sfit <- lm(Sales ~ Price + US, data = Carseats)
summary(sfit)
```

## 10f
Model (e) is a little better than (a) (0.2354 vs. 0.2335)

## 10g
```{r}
confint(sfit)
```
Compared with 8(a).iv, compute the confidence/prediction intervals of **response variable** with 
`predict(model, data.frame(predictor = c(v1, v2, ...)), interval = 'confidence/prediction')`. Compute confidence interval of **coefficients** with function `confint(model)`.

## 10h
```{r}
plot(predict(sfit), rstudent(sfit))
par(mfrow=c(2,2))
plot(sfit)
```
So in the model (e), there is no outliers.

For leverage check, $(p + 1) / n = 3 / 400 = 0.0075$
So there is no points with high leverage (larger than 0.0075).

Reference: The last paragraph in p98:

> So if a given observation has a leverage statistic that greatly exceeds $(p+1)/n$, then we may suspect that the corresponding point has high leverage.

# Question 11
## 11a
```{r}
set.seed(1)
x <- rnorm(100)
y <- 2 * x + rnorm(100)
fit <- lm(y ~ x + 0)
summary(fit)
```

So the coefficient estimate $\hat \beta = 1.99$. Its standard error is 0.1065.
The t-statistic is 18.73, and its p-value is less than $2^{-16}$.
The regression result, 1.99, is very close to the theoretical value, which is 2.

## 11b
```{r}
fitr <- lm(x ~ y + 0)
summary(fitr)
```

The coefficient estimate $\hat \beta = 0.39$, its standard error is 0.02.
The t-stats and its p-value is 18.73 and $\lt 2^{-16}$.
The regression result, 0.39, is less than theoretical value $\frac12$.

## 11c
The coefficients in (a) and (b) should be reciprocal, because
$$
y = 2x + \epsilon \\
\Rightarrow x = \frac{y - \epsilon}2 \\
\Rightarrow x = \frac12 y - \frac{\epsilon}2 \\
\therefore \hat\beta = \frac12
$$
## 11d

$$\begin{aligned}
SE(\hat\beta) &= \sqrt{\frac{\sum_{i=1}^n(y_i - x_i \hat\beta)^2}{(n-1)\sum_{i=1}^n x_i^2}} \\
&= \sqrt{\frac{\sum_{i=1}^n y_i^2 - 2\hat\beta\sum_{i=1}^nx_iy_i + \hat\beta^2\sum_{i=1}^nx_i^2}{(n-1)\sum_{i=1}^n x_i^2}} \\
\end{aligned}
$$
Take this into (3.14), we have:
$$ \begin{aligned}
t &= \frac{\hat\beta}{SE(\hat\beta)} \\
&= \frac{\hat\beta\sqrt{(n-1)\sum_{i=1}^n x_i^2}}{\sqrt{\sum_{i=1}^n y_i^2 - 2\hat\beta\sum_{i=1}^nx_iy_i + \hat\beta^2\sum_{i=1}^nx_i^2}} \\
&= \sqrt{\frac{(n-1)\sum_{i=1}^nx_i^2}{\frac{\sum_{i=1}^n y_i^2 - 2\hat\beta\sum_{i=1}^nx_iy_i + \hat\beta^2\sum_{i=1}^nx_i^2}{\hat\beta^2}}} \\
&= \sqrt{\frac{(n-1)\sum_{i=1}^nx_i^2}{\frac{\sum_{i=1}^n y_i^2}{\hat\beta^2} - 2\frac1{\hat\beta}\sum_{i=1}^nx_iy_i + \sum_{i=1}^nx_i^2}} \\
&= \sqrt{\frac{n-1}{\frac{\sum_{i=1}^ny_i^2}{\hat\beta^2 \sum_{i=1}^n x_i^2} -2\frac{\sum_{i=1}^n x_i y_i}{\hat\beta\sum_{i=1}^n x_i^2} + 1}}
\end{aligned}
$$

Take (3.38) into above equation, we have:
$$ \begin{aligned}
t &= \sqrt{\frac{n-1}{\frac{\sum_{i=1}^nx_i^2 \sum_{i=1}^ny_i^2}{(\sum_{i=1}^n x_i y_i)^2} - 2 + 1}} \\

&= \sqrt {
  \frac{ (n-1)(\sum_{i=1}^n x_i y_i)^2}
       { \sum_{i=1}^n x_i^2 \sum_{i=1}^n y_i^2 
       - (\sum_{i=1}^n x_i y_i)^2}
} \\

&= \frac { \sqrt{n-1}
           \sum_{i=1}^n x_i y_i
         }
         { \sqrt{ \sum_{i=1}^n x_i^2 \sum_{i=1}^n y_i^2 
                  - (\sum_{i=1}^n x_i y_i)^2
                }
         }
\end{aligned}
$$

Proof complete.

$t$ calculated in R:
```{r}
sqrt(length(x) -1) * sum(x*y) / sqrt(sum(x^2)*sum(y^2) - (sum(x * y))^2)
```

18.72593 vs 18.73 in 11a.

## 11e

From the $t$ expression above, it's easy to find that $x$ and $y$ are symmetric,
which means the result keeps the same when we swap $x$ and $y$.
So the t-statistic for the regression of $y$ onto $x$ is the same as the t-statistic for the regression of $x$ onto $y$.

## 11f

```{r}
summary(lm(y ~ x))
summary(lm(x ~ y))
```
So the t-statistic are 18.556 and 18.56.

# Question 12

## 12a

Based on (3.38), to keep the coefficient the same, we have:
$$
\frac{\sum_{i=1}^n x_i y_i}{\sum_{j=1}^n x_j^2} = \frac{\sum_{i=1}^n x_i y_i}{\sum_{j=1}^n y_j^2} \\
\therefore \sum_{i=1}^n x_i^2 = \sum_{i=1}^n y_i^2
$$
So when sum of squares of observed $y$ is equals to sum of squares of observed $x$, the $\hat\beta$ is the same.

## 12b
The same with 11a.

## 12c
```{r}
set.seed(1)
x <- rnorm(100)
y <- sample(x, 100)
print(sum(x ^ 2) == sum(y ^ 2))
summary(lm(y ~ x + 0))
summary(lm(x ~ y + 0))
```


# Question 13
## 13a-c
```{r}
set.seed(1)
x <- rnorm(100)
eps <- rnorm(100, sd = 0.25)
y <- -1 + 0.5 * x + eps
length(y)
```
Length of the vecto $y$ is 100. $\beta_0 = -1$, $\beta_1 = 0.5$.

## 13d
```{r}
plot(x, y)
```
With bare eyes we can see when $x=0$, $y \approx -1$, and $y \approx 0 \lvert x = 2$, which matches the line $y = -1 + 0.5 x$.

## 13e
```{r}
f13e <- lm(y ~ x)
summary(f13e)
```
So $\beta_0 = -1$ vs. $\hat\beta_0 = -1.009$, $\beta_1 = 0.5$ vs $\hat\beta_1 = 0.4997$.

## 13f
```{r}
plot(x, y)
abline(f13d, col = 'red', lty = 4)
abline(-1, 0.5, col = 'blue', lty = 2)
legend('bottomright', c('population regression', 'least square'), lty = c(4,2), col = c('red', 'blue'), bty = 'n')
```
See figure 3.3 for reference.

## 13g

```{r}
f13g <- lm(y ~ poly(x, 2))
summary(f13g)
```
$R^2$ statistic from 0.7762 to 0.7784 shows that the quadratic term has almost no improvements for the model fit.

## 13h
```{r}
set.seed(1)
x <- rnorm(100)
eps <- rnorm(100, sd = 0.1)
y <- -1 + 0.5 * x + eps
f13h <- lm(y ~ x)
summary(f13h)
plot(x, y)
abline(f13h, col = 'red', lty = 4)
abline(-1, 0.5, col = 'blue', lty = 2)
legend('bottomright', c('population regression', 'least square'), lty = c(4,2), col = c('red', 'blue'), bty = 'n')
```

The observations are more closed to the population regression line.
$R^2$ statistic increases from 0.78 to 0.96.

## 13i
```{r}
set.seed(1)
x <- rnorm(100)
eps <- rnorm(100, sd = 0.5)
y <- -1 + 0.5 * x + eps
f13i <- lm(y ~ x)
summary(f13i)
plot(x, y)
abline(f13i, col = 'red', lty = 4)
abline(-1, 0.5, col = 'blue', lty = 2)
legend('bottomright', c('population regression', 'least square'), lty = c(4,2), col = c('red', 'blue'), bty = 'n')
```
The observations are more disperse from the population regression line.
$R^2$ statistic decreases from 0.78 to 0.46.

## 13j
```{r}
confint(f13e)
confint(f13h)
confint(f13i)
```
We can see that as the noises in the observations increase, the confidence intervals become much wider, but the mean value keep the same.

# Question 14

## 14a

Perform the following commands in R:
```{r}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100) / 10
y <- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)
```

The last line corresponds to creating a linear model in which $y$ is a function of $x1$ and $x2$.
Write out the form of the linear model. What are the regression coefficients?

The linear model is:
$$
y = 2 + 2x_1 + 0.3x_2 + \epsilon
$$

The coefficients are:
$$
\beta_0 = 2 \\
\beta_1 = 2 \\
\beta_2 = 0.3
$$

## 14b

What is the correlation between x1 and x2?
Create a scatterplot displaying the relationship between the variables.

```{r}
cor(x1, x2)
plot(x1, x2)
```

## 14c

Using this data, fit a least squares regression to predict $y$ using $x1$ and $x2$.
Describe the results obtained. What are $\hat \beta_0$, $\hat \beta_1$, and $\hat \beta_2$?
How do these relate to the true $\beta_0$, $\beta_1$, and $\beta_2$?
Can you reject the null hypothesis $H_0 : \beta_1 = 0$?
How about the null hypothesis $H_0 : \beta_2 = 0$?
```{r}
f14c <- lm(y ~ x1 + x2)
summary(f14c)
```
$\hat\beta_0 = 2.13$, $\hat\beta_1 = 1.44$, $\hat\beta_2 = 1.01$.
$\hat\beta_1$ and $\hat\beta_2$ are both far from the true coefficients (2 and 0.3, respectively).

From the p-values of the t-statistic(0.0487 and 0.3754), we can't reject the null hypothesis of $\beta_1 = 0$ and $\beta_2 = 0$ (for 0.0487 is so close to 0.05).

## 14d

Now fit a least squares regression to predict $y$ using only $x1$.
Comment on your results. Can you reject the null hypothesis $H_0 : \beta_1 = 0$?
```{r}
f14d <- lm(y ~ x1)
summary(f14d)
```
Accroding to the p-value of t-statistic, we can reject the $H_0$ hypothesis.

## 14e

Now fit a least squares regression to predict $y$ using only $x2$.
Comment on your results. Can you reject the null hypothesis $H_0 : \beta_1 = 0$?
```{r}
f14e <- lm(y ~ x2)
summary(f14e)
```
Accroding to the p-value of t-statistic, we can reject the $H_0$ hypothesis.

## 14f

Do the results obtained in (c)â€“(e) contradict each other? Explain your answer.

No. The results don't contradict each other.
Because when the independent variables are correlated to each other,
they can't be fitted with linear regression model.

## 14g

Now suppose we obtain one additional observation, which was unfortunately mismeasured.

```{r}
x1n <- c(x1, 0.1)
x2n <- c(x2, 0.8)
yn <- c(y, 6)
```

Re-fit the linear models from (c) to (e) using this new data.
What effect does this new observation have on the each of the models?
In each model, is this observation an outlier? A high-leverage point? Both?
Explain your answers.

```{r}
f14g <- lm(yn ~ x1n + x2n)
summary(f14g)
par(mfrow=c(2,2))
plot(f14g)
par(mfrow=c(1,1))
plot(predict(f14g), rstudent(f14g))
```

This observation disturbs the relationship between $y$ and $x1$, $x2$ severely.
The coefficients become more far from the true values.
From these plots we can see, it is both outlier and with high-leverage.

```{r}
f14g2 <- lm(yn ~ x1n)
summary(f14g2)
par(mfrow=c(2,2))
plot(f14g2)
par(mfrow=c(1,1))
plot(predict(f14g2), rstudent(f14g2))
points(predict(f14g2)[101], rstudent(f14g2)[101], col = 'red', cex = 2, pch = 3)
```
The $x1n$ has no statistical relation with $y$ because of the 101th observation.
From these plots we can see, this observation is both outlier and with high-leverage.

```{r}
f14g3 <- lm(yn ~ x2n)
summary(f14g3)
par(mfrow=c(2,2))
plot(f14g3)
par(mfrow=c(1,1))
plot(predict(f14g3), rstudent(f14g3))
```
The $x2n$ still has statistical relation with the $y$. But the $R^2$ is very low.
From these plots we can see, this observation is both outlier and with high-leverage.

# Question 15
## 15a
```{r}
library(MASS)
names(Boston)
f15a1 <- lm(crim ~ zn, data = Boston)
f15a2 <- lm(crim ~ indus, data = Boston)
f15a3 <- lm(crim ~ chas, data = Boston)
f15a4 <- lm(crim ~ nox, data = Boston)
f15a5 <- lm(crim ~ rm, data = Boston)
f15a6 <- lm(crim ~ age, data = Boston)
f15a7 <- lm(crim ~ dis, data = Boston)
f15a8 <- lm(crim ~ rad, data = Boston)
f15a9 <- lm(crim ~ tax, data = Boston)
f15a10 <- lm(crim ~ ptratio, data = Boston)
f15a11 <- lm(crim ~ black, data = Boston)
f15a12 <- lm(crim ~ lstat, data = Boston)
f15a13 <- lm(crim ~ medv, data = Boston)
summary(f15a1)
summary(f15a2)
summary(f15a3)
summary(f15a4)
summary(f15a5)
summary(f15a6)
summary(f15a7)
summary(f15a8)
summary(f15a9)
summary(f15a10)
summary(f15a11)
summary(f15a12)
summary(f15a13)
par(mfrow=c(2,2))
plot(f15a1)
```
All predictors except *chas* has statistically significant associations with *crim*.

## 15b
```{r}
f15b <- lm(crim ~ ., data = Boston)
summary(f15b)
```
For the predictors *zn*, *dis*, *rad*, *medv* and *black* we can reject the null hypothesis.

## 15c
```{r}
sim.coef <- c(coef(f15a1)[2], coef(f15a2)[2], coef(f15a3)[2], coef(f15a4)[2], coef(f15a5)[2], coef(f15a6)[2], coef(f15a7)[2], coef(f15a8)[2], coef(f15a9)[2], coef(f15a10)[2], coef(f15a11)[2], coef(f15a12)[2], coef(f15a13)[2])
mul.coef <- coef(f15b)[-1]
plot(sim.coef, mul.coef)
```
The predictor *nox* has positive influence no *crim* in simple linear regression and negative influence in multiple linear regression. The former says when calculating the average value of  crime rates according to nitrogen oxides, the *crim* increate as the *nox* increase. The latter says when other predictors fixed, the crime rates decrease as the nitrogen oxides increase.
Section 4.3.4 on page 135 provides detailed explanations about a similar problem:
> In general, the phenomenon seen in Figure 4.3 is known as **confounding**.

## 15d
```{r}
f15d1 <- lm(crim ~ poly(zn, 3), data = Boston)
f15d2 <- lm(crim ~ poly(indus, 3), data = Boston)
# f15d3 <- lm(crim ~ poly(chas, 3), data = Boston)
f15d4 <- lm(crim ~ poly(nox, 3), data = Boston)
f15d5 <- lm(crim ~ poly(rm, 3), data = Boston)
f15d6 <- lm(crim ~ poly(age, 3), data = Boston)
f15d7 <- lm(crim ~ poly(dis, 3), data = Boston)
f15d8 <- lm(crim ~ poly(rad, 3), data = Boston)
f15d9 <- lm(crim ~ poly(tax, 3), data = Boston)
f15d10 <- lm(crim ~ poly(ptratio, 3), data = Boston)
f15d11 <- lm(crim ~ poly(black, 3), data = Boston)
f15d12 <- lm(crim ~ poly(lstat, 3), data = Boston)
f15d13 <- lm(crim ~ poly(medv, 3), data = Boston)
summary(f15d1)
summary(f15d2)
# summary(f15d3)
summary(f15d4)
summary(f15d5)
summary(f15d6)
summary(f15d7)
summary(f15d8)
summary(f15d9)
summary(f15d10)
summary(f15d11)
summary(f15d12)
summary(f15d13)
```

So the *black* predictor has no non-linear relationship with *crim*.
*lstat* has a subtle non-linear relatonship with *crim*.
*chas* has no linear relationship with *crim* (see 15a), hence no non-linear relationship, too.
Other predictors have different levels but significant non-linear relationship with *crim*.